# VERIFY: 4-Layer Validation System

**목표:** Hallucination 방지, 모든 사실적 주장에 근거 확보

**원칙:** "출처 없는 주장 = 무효"

---

## Layer 1: Source Grounding (출처 기반)

### 규칙

**모든 사실적 주장에 출처 필수**

- ✅ 출처 있음: 유효한 주장
- ❌ 출처 없음: **[?]** 태그 + "모른다" 표현

### 예시

```markdown
✅ 좋은 예:
"GPT-4는 2023년 3월 14일 출시되었다 (openai.com)"

❌ 나쁜 예:
"GPT-4는 2023년 3월 14일 출시되었다" ← 출처 없음
→ "GPT-4는 2023년 3월에 출시된 것으로 보인다 [?]"
   또는 "GPT-4 출시일을 모르겠습니다"
```

### 출처 종류별 신뢰도

| 출처 종류 | 신뢰도 | 예시 |
|----------|--------|------|
| **Peer-reviewed 논문** | 0.9-0.95 | Nature, Science, IEEE |
| **공식 발표** | 0.85-0.9 | 기업 공식 블로그, 정부 발표 |
| **Preprint** | 0.7-0.8 | arXiv (미검토) |
| **뉴스 (전문)** | 0.7-0.75 | The Verge, Ars Technica |
| **뉴스 (일반)** | 0.6-0.7 | Forbes, TechCrunch |
| **블로그** | 0.4-0.6 | 개인 블로그, Medium |
| **포럼** | 0.3-0.5 | Reddit, HackerNews |
| **출처 없음** | 0.0-0.3 | [?] 태그 |

### "모른다" 표현하기

**중요:** 모르면 솔직히 인정

```markdown
좋은 표현:
- "출처를 찾지 못했습니다"
- "현재 정보로는 확인 불가능합니다"
- "추가 검색이 필요합니다"

나쁜 표현:
- "아마도 ~일 것입니다" (추측)
- "일반적으로 ~합니다" (모호함)
- "~로 알려져 있습니다" (출처 불명)
```

---

## Layer 2: Cross-Validation (교차 검증)

### 규칙

**여러 독립 출처로 교차 확인**

### 신뢰도 공식

| 소스 수 | 일치 여부 | 신뢰도 |
|---------|----------|--------|
| **1개 소스** | - | 0.6 (낮음) |
| **2개 소스** | 일치 | 0.8 (중간) |
| **3개+ 소스** | 일치 | 0.95 (높음) |
| **2개+ 소스** | 모순 | ⚠️ 플래그 |

### 예시

#### 단일 소스 (신뢰도 0.6)

```markdown
"Transformer 아키텍처는 2017년 제안되었다 (Wikipedia)"

→ 신뢰도: 0.6 (단일 소스)
→ 태그: ✓ HIGH
```

#### 2개 소스 일치 (신뢰도 0.8)

```markdown
"Transformer 아키텍처는 2017년 제안되었다"
- 소스 1: arxiv.org/abs/1706.03762 (원 논문)
- 소스 2: deeplearning.ai (교육 자료)

→ 신뢰도: 0.8 (2개 소스 일치)
→ 태그: ✓✓ VERIFIED
```

#### 3개+ 소스 일치 (신뢰도 0.95)

```markdown
"GPT-4는 2023년 3월 14일 출시되었다"
- 소스 1: openai.com (공식)
- 소스 2: techcrunch.com (뉴스)
- 소스 3: the verge.com (뉴스)

→ 신뢰도: 0.95 (3개 소스 일치)
→ 태그: ✓✓ VERIFIED
```

#### 모순 발견 (⚠️)

```markdown
"양자 컴퓨터는 2030년까지 실용화된다"
- 소스 1: IBM 블로그 → "2030년 실용화 예상"
- 소스 2: Nature 논문 → "2040년 이후 가능"
- 소스 3: 전문가 인터뷰 → "10-20년 소요"

→ ⚠️ CONTRADICTED (모순)
→ 추가 조사 필요
```

### 독립성 체크

**중요:** 같은 원본을 인용한 출처는 독립 아님

```markdown
❌ 독립 아님:
- 소스 1: TechCrunch (원본)
- 소스 2: Forbes (TechCrunch 인용)
- 소스 3: Medium (TechCrunch 재인용)
→ 실제로는 1개 소스

✅ 독립적:
- 소스 1: OpenAI 공식 발표
- 소스 2: Google Research 독립 검증
- 소스 3: Stanford 연구팀 실험
→ 3개 독립 소스
```

---

## Layer 3: Self-Consistency (자기 일관성)

### 규칙

**중요 결론은 다른 각도에서 재검토**

### 방법

#### 1. 역방향 검증

**정방향 주장:**
"A이므로 B이다"

**역방향 검증:**
"B가 맞다면, A도 맞아야 한다. A가 맞는가?"

**예시:**
```markdown
정방향:
"양자 컴퓨터는 소인수분해가 빠르므로, RSA 암호화를 위협한다"

역방향 검증:
"RSA가 위협받으려면, 충분한 큐비트 + 낮은 오류율 필요하다.
 현재 양자 컴퓨터가 이를 만족하는가?"
→ 확인: 큐비트 수 부족 + 오류율 높음
→ 결론 수정: "이론적으로는 위협, 실용적으로는 아직 아님"
```

#### 2. 대안 설명 검토

**현재 설명:**
[A가 B의 원인이다]

**대안 설명:**
- C가 B의 원인일 수도?
- D가 B의 원인일 수도?

**최선 설명 선택:**
- 증거가 가장 많은 것
- 가장 단순한 것 (오컴의 면도날)

**예시:**
```markdown
현상: AI 성능 향상

설명 1: 데이터 증가 (증거: ✓✓)
설명 2: 모델 크기 증가 (증거: ✓✓)
설명 3: 알고리즘 개선 (증거: ✓)
설명 4: 하드웨어 발전 (증거: ✓)

→ 여러 요인 복합 작용 (단일 원인 아님)
```

#### 3. 시간적 일관성

**과거 주장:**
[iteration N에서 주장한 것]

**현재 주장:**
[iteration N+5에서 주장하는 것]

**일관성 체크:**
- 모순되는가?
- 모순된다면, 새 증거 때문인가 아니면 실수인가?

**예시:**
```markdown
Iteration 3:
"양자 컴퓨터는 5년 내 실용화 가능"

Iteration 10:
"양자 컴퓨터는 10년 이상 소요"

→ 모순! 왜?
→ 새 증거: IBM 오류율 개선 실패 뉴스 (Nature)
→ 일관성 OK (새 증거 기반 업데이트)
```

---

## Layer 4: Confidence Tagging (신뢰도 태깅)

### 태그 시스템

| 태그 | 의미 | 조건 |
|------|------|------|
| **✓✓ VERIFIED** | 검증됨 | 3개+ 독립 소스, 신뢰도 >0.9 |
| **✓ HIGH** | 높음 | 1-2개 신뢰 소스, 신뢰도 0.7-0.9 |
| **~ LIKELY** | 가능성 | 추정, 신뢰도 0.5-0.7 |
| **? UNCERTAIN** | 불확실 | 출처 부족, 신뢰도 <0.5 |
| **⚠ CONTRADICTED** | 모순 | 소스 간 충돌 |

### 사용 예시

```markdown
## 핵심 발견

1. ✓✓ GPT-4는 Transformer 아키텍처 기반이다
   - 소스: openai.com, arxiv.org, nature.com
   - 신뢰도: 0.98

2. ✓ GPT-4는 약 1.7T 파라미터를 사용한다
   - 소스: techcrunch.com (비공식 추정)
   - 신뢰도: 0.75

3. ~ GPT-4 학습 비용은 약 $100M으로 추정된다
   - 소스: forbes.com (전문가 추정)
   - 신뢰도: 0.60

4. ? GPT-5는 2025년 출시 예정이다
   - 소스: 없음 (소문)
   - 신뢰도: 0.30

5. ⚠ GPT-4는 AGI를 달성했다
   - 소스 A: wired.com (일부 전문가 주장)
   - 소스 B: nature.com (대다수 전문가 반대)
   - 신뢰도: 모순
```

---

## 검증 체크리스트

### 전체 체크

```markdown
□ **Layer 1**: 모든 주장에 출처 있는가?
  - 출처 없는 주장 → [?] 태그 또는 "모른다"

□ **Layer 2**: 교차 검증했는가?
  - 1개 소스 → 0.6 신뢰도
  - 2개 소스 일치 → 0.8
  - 3개+ 소스 일치 → 0.95
  - 모순 → ⚠️ 플래그

□ **Layer 3**: 자기 일관성 확인했는가?
  - 역방향 검증
  - 대안 설명 고려
  - 시간적 일관성

□ **Layer 4**: 신뢰도 태그 부여했는가?
  - ✓✓ / ✓ / ~ / ? / ⚠️
```

### 개별 주장 체크

```markdown
주장: [사실적 주장]

□ 출처가 있는가? → [예/아니오]
  - 아니오 → [?] 태그 또는 제거

□ 출처가 신뢰할 만한가? → [신뢰도 0.X]
  - Peer-reviewed? 공식? 블로그?

□ 교차 검증했는가? → [소스 X개]
  - 1개 → 0.6
  - 2개 → 0.8
  - 3개+ → 0.95

□ 모순은 없는가? → [예/아니오]
  - 예 → ⚠️ 플래그

□ 자기 일관성? → [예/아니오]
  - 역방향, 대안, 시간

□ 최종 태그: [✓✓ / ✓ / ~ / ? / ⚠️]
```

---

## 자주 하는 실수

### ❌ 잘못된 검증

1. **출처 없이 확신**
   ```
   "GPT-4는 1.7T 파라미터다"
   → 출처는? OpenAI 공식 발표 없음!
   ```

2. **단일 소스로 VERIFIED 태깅**
   ```
   ✓✓ "양자 컴퓨터는 2030년 실용화" (IBM 블로그)
   → 단일 소스인데 ✓✓? → ✓로 수정
   ```

3. **모순 무시**
   ```
   소스 A: "5년" vs 소스 B: "20년"
   → "평균 12.5년" ← 잘못된 처리!
   → ⚠️ CONTRADICTED 태그 + 추가 조사
   ```

4. **블로그를 peer-reviewed처럼 신뢰**
   ```
   "Medium 블로그에 따르면..." → 신뢰도 0.9
   → 블로그는 0.4-0.6!
   ```

### ✅ 올바른 검증

1. **출처 명시**
   ```
   "GPT-4는 2023년 3월 14일 출시 (openai.com, techcrunch.com)"
   ```

2. **적절한 태깅**
   ```
   ✓ "IBM 블로그에 따르면 2030년 실용화 예상"
   (단일 소스이므로 ✓)
   ```

3. **모순 명시**
   ```
   ⚠️ "실용화 시점: 5년(IBM) vs 20년(Nature 논문)"
   → 추가 조사 필요
   ```

4. **출처별 신뢰도**
   ```
   Nature 논문 (0.95) > TechCrunch (0.7) > Medium (0.5)
   ```

---

## 종합 예시

```markdown
## 검증 결과

### ✓✓ VERIFIED (신뢰도 0.95+)

1. Transformer 아키텍처는 2017년 "Attention Is All You Need" 논문에서 제안
   - 소스: arxiv.org/abs/1706.03762 (원 논문)
   - 소스: Google Research 블로그
   - 소스: Stanford CS231n 강의 자료
   - 교차 검증: 3개 독립 소스 일치
   - 자기 일관성: OK

### ✓ HIGH (신뢰도 0.7-0.9)

2. GPT-4는 약 1.7T 파라미터 사용 추정
   - 소스: TechCrunch (비공식 추정)
   - 소스: The Verge (업계 분석)
   - 교차 검증: 2개 소스 유사 (1.7-1.8T)
   - 주의: OpenAI 공식 발표 없음

### ~ LIKELY (신뢰도 0.5-0.7)

3. GPT-4 학습 비용 $100M 추정
   - 소스: Forbes (전문가 추정)
   - 교차 검증: 단일 소스
   - 주의: 공식 데이터 없음, 추정치

### ? UNCERTAIN (신뢰도 <0.5)

4. GPT-5 2025년 출시?
   - 소스: 없음 (소문, 추측)
   - 신뢰도: 매우 낮음
   - 결론: 확인 불가

### ⚠️ CONTRADICTED (모순)

5. GPT-4가 AGI 달성했는가?
   - 소스 A: Wired (일부 전문가 YES)
   - 소스 B: Nature (대다수 전문가 NO)
   - 소스 C: OpenAI (명시적 부인)
   - 결론: 컨센서스는 "아니오", 소수 의견만 "예"
```

---

**VERIFY 완료 → SYNTHESIZE 단계로 진행**
